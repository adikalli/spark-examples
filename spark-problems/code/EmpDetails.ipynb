{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e55932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d7e5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/11/07 16:29:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10664c3d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c8845f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "929d26c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime\n",
    "from faker.providers.person.en import Provider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cb1d3f",
   "metadata": {},
   "source": [
    "**Create a Employee table containing below columns. (1000 records) <br>\n",
    "Employee Id, Employee Name,DOB, Job title , Job Start Date, Level ( bw 1 to 5)<br><br>\n",
    "Create a Salary table Employee Id, Salary (bw 1L to 20L)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de28e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=1000\n",
    "start_id=1000\n",
    "emp_id=list(range(start_id,start_id+sample_size))\n",
    "emp_name=list(set(Provider.first_names))[0:sample_size]\n",
    "dob=[datetime.date(random.randint(1982,2000), random.randint(1,12),random.randint(1,28)) for i in range(sample_size)]\n",
    "job_title=random.choices(['Data Engineer','Sr Data Engineer','Data Scientist','Sr Data Scientist','Manager'],k=sample_size)\n",
    "job_start_date=[datetime.date(random.randint(2012,2022), random.randint(1,12),random.randint(1,28)) for i in range(sample_size)]\n",
    "level=[random.randrange(1, 6) for i in range(sample_size)]\n",
    "salary=[random.randint(1, 20)*100000 for i in range(sample_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42838293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+-----------------+--------------+-----+\n",
      "|emp_id|emp_name|       dob|        job_title|job_start_date|level|\n",
      "+------+--------+----------+-----------------+--------------+-----+\n",
      "|  1000|    Bunk|1991-05-10|    Data Engineer|    2019-08-10|    1|\n",
      "|  1001|  Dennis|1993-01-16| Sr Data Engineer|    2020-08-01|    5|\n",
      "|  1002|   Gaven|1991-09-25| Sr Data Engineer|    2019-02-08|    4|\n",
      "|  1003|  Aliyah|1993-05-15|Sr Data Scientist|    2016-11-06|    3|\n",
      "|  1004|    Nola|1993-05-13|          Manager|    2022-09-08|    3|\n",
      "+------+--------+----------+-----------------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-------+\n",
      "|emp_id| salary|\n",
      "+------+-------+\n",
      "|  1000|1500000|\n",
      "|  1001|1100000|\n",
      "|  1002| 700000|\n",
      "|  1003| 400000|\n",
      "|  1004|1200000|\n",
      "+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp=sc.parallelize(list(zip(emp_id,emp_name,dob,job_title,job_start_date,level)))\\\n",
    "            .toDF((\"emp_id\", \"emp_name\", \"dob\",\"job_title\",\"job_start_date\",\"level\"))\n",
    "df_emp.show(5)\n",
    "df_sal=sc.parallelize(list(zip(emp_id,salary))).toDF((\"emp_id\",\"salary\"))\n",
    "df_sal.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dea533b",
   "metadata": {},
   "source": [
    "**Calculate years of experience and Age. (Use UDF )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f48221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "@udf('int')\n",
    "def my_date_udf(dt):\n",
    "    from datetime import date\n",
    "    curr = date.today()\n",
    "    delta = (curr-dt).total_seconds()\n",
    "    return round(int(delta/(60*60*24*365)),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc491a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+-----------------+--------------+-----+---+---------+\n",
      "|emp_id|emp_name|       dob|        job_title|job_start_date|level|age|years_exp|\n",
      "+------+--------+----------+-----------------+--------------+-----+---+---------+\n",
      "|  1000|    Bunk|1991-05-10|    Data Engineer|    2019-08-10|    1| 31|        3|\n",
      "|  1001|  Dennis|1993-01-16| Sr Data Engineer|    2020-08-01|    5| 29|        2|\n",
      "|  1002|   Gaven|1991-09-25| Sr Data Engineer|    2019-02-08|    4| 31|        3|\n",
      "|  1003|  Aliyah|1993-05-15|Sr Data Scientist|    2016-11-06|    3| 29|        6|\n",
      "|  1004|    Nola|1993-05-13|          Manager|    2022-09-08|    3| 29|        0|\n",
      "|  1005|  Karina|1982-12-11|    Data Engineer|    2013-11-20|    2| 39|        8|\n",
      "|  1006|    Chas|1993-03-21|   Data Scientist|    2022-03-25|    1| 29|        0|\n",
      "|  1007|    Nova|1993-07-05| Sr Data Engineer|    2017-03-21|    5| 29|        5|\n",
      "|  1008|    Nena|1988-12-01|          Manager|    2022-05-16|    4| 33|        0|\n",
      "|  1009|   Lavar|1994-03-04|    Data Engineer|    2016-02-25|    2| 28|        6|\n",
      "|  1010|   Codey|1999-01-21| Sr Data Engineer|    2022-03-05|    4| 23|        0|\n",
      "|  1011|    Adah|1994-11-02|Sr Data Scientist|    2017-03-14|    3| 28|        5|\n",
      "|  1012| Marques|1986-01-25|Sr Data Scientist|    2017-10-16|    4| 36|        5|\n",
      "|  1013|Berneice|1988-05-28|    Data Engineer|    2022-10-23|    1| 34|        0|\n",
      "|  1014| Florida|1987-03-27|    Data Engineer|    2013-03-04|    1| 35|        9|\n",
      "|  1015|   Kisha|1989-11-03|   Data Scientist|    2017-03-13|    3| 33|        5|\n",
      "|  1016|   Dayna|1999-03-25|Sr Data Scientist|    2021-10-27|    5| 23|        1|\n",
      "|  1017|   Suzie|1993-12-23|Sr Data Scientist|    2017-04-14|    3| 28|        5|\n",
      "|  1018|    Kirk|1991-07-23|    Data Engineer|    2014-04-27|    5| 31|        8|\n",
      "|  1019|  Helena|1994-12-19|    Data Engineer|    2020-10-01|    1| 27|        2|\n",
      "+------+--------+----------+-----------------+--------------+-----+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,datediff,current_date,floor\n",
    "\n",
    "#Using Built-in datediff function\n",
    "# df_emp.withColumn('Age',floor(datediff(current_date(),col('dob'))/365))\\\n",
    "#         .withColumn('years_exp',floor(datediff(current_date(),col('job_start_date'))/365)).show()\n",
    "\n",
    "#Using UDF\n",
    "df_1=df_emp.withColumn('age',my_date_udf(col('dob')))\\\n",
    "        .withColumn('years_exp',my_date_udf(col('job_start_date')))\n",
    "\n",
    "df_1.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7b865e",
   "metadata": {},
   "source": [
    "**Create a flag EligibleForBonus if the years of experience is greater than 2 and salary is below 10L.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b97b46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+-----------------+--------------+-----+---+---------+-------+----------------+\n",
      "|emp_id|emp_name|       dob|        job_title|job_start_date|level|age|years_exp| salary|EligibleForBonus|\n",
      "+------+--------+----------+-----------------+--------------+-----+---+---------+-------+----------------+\n",
      "|  1010|   Codey|1999-01-21| Sr Data Engineer|    2022-03-05|    4| 23|        0| 500000|               N|\n",
      "|  1002|   Gaven|1991-09-25| Sr Data Engineer|    2019-02-08|    4| 31|        3| 700000|               Y|\n",
      "|  1016|   Dayna|1999-03-25|Sr Data Scientist|    2021-10-27|    5| 23|        1|1400000|               N|\n",
      "|  1012| Marques|1986-01-25|Sr Data Scientist|    2017-10-16|    4| 36|        5| 900000|               Y|\n",
      "|  1009|   Lavar|1994-03-04|    Data Engineer|    2016-02-25|    2| 28|        6| 900000|               Y|\n",
      "|  1000|    Bunk|1991-05-10|    Data Engineer|    2019-08-10|    1| 31|        3|1500000|               N|\n",
      "|  1013|Berneice|1988-05-28|    Data Engineer|    2022-10-23|    1| 34|        0|1400000|               N|\n",
      "|  1007|    Nova|1993-07-05| Sr Data Engineer|    2017-03-21|    5| 29|        5|1000000|               N|\n",
      "|  1011|    Adah|1994-11-02|Sr Data Scientist|    2017-03-14|    3| 28|        5| 500000|               Y|\n",
      "|  1005|  Karina|1982-12-11|    Data Engineer|    2013-11-20|    2| 39|        8| 100000|               Y|\n",
      "|  1001|  Dennis|1993-01-16| Sr Data Engineer|    2020-08-01|    5| 29|        2|1100000|               N|\n",
      "|  1018|    Kirk|1991-07-23|    Data Engineer|    2014-04-27|    5| 31|        8| 300000|               Y|\n",
      "|  1020|   Gauge|1992-02-04|    Data Engineer|    2020-01-22|    2| 30|        2|1600000|               N|\n",
      "|  1015|   Kisha|1989-11-03|   Data Scientist|    2017-03-13|    3| 33|        5| 800000|               Y|\n",
      "|  1014| Florida|1987-03-27|    Data Engineer|    2013-03-04|    1| 35|        9| 700000|               Y|\n",
      "|  1019|  Helena|1994-12-19|    Data Engineer|    2020-10-01|    1| 27|        2| 600000|               N|\n",
      "|  1008|    Nena|1988-12-01|          Manager|    2022-05-16|    4| 33|        0|1900000|               N|\n",
      "|  1004|    Nola|1993-05-13|          Manager|    2022-09-08|    3| 29|        0|1200000|               N|\n",
      "|  1006|    Chas|1993-03-21|   Data Scientist|    2022-03-25|    1| 29|        0| 200000|               N|\n",
      "|  1003|  Aliyah|1993-05-15|Sr Data Scientist|    2016-11-06|    3| 29|        6| 400000|               Y|\n",
      "+------+--------+----------+-----------------+--------------+-----+---+---------+-------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,when,lit\n",
    "\n",
    "df_2=df_1.join(df_sal,on='emp_id',how='left')\\\n",
    "    .withColumn('EligibleForBonus',when( ((col('years_exp')>2) & (col('salary')<1000000)), lit('Y') )\\\n",
    "                                  .otherwise(lit('N')))\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc6ad8",
   "metadata": {},
   "source": [
    "**For employees eligible for Bonus, increment salary by 5% by each level i.e 5% for level 1 ,10% for level2 and so on. Save the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bebdd8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 1, 3, 2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+-----------------+--------------+-----+---+---------+-------+----------------+-------+\n",
      "|emp_id|emp_name|       dob|        job_title|job_start_date|level|age|years_exp| salary|EligibleForBonus|new_sal|\n",
      "+------+--------+----------+-----------------+--------------+-----+---+---------+-------+----------------+-------+\n",
      "|  1010|   Codey|1999-01-21| Sr Data Engineer|    2022-03-05|    4| 23|        0| 500000|               N| 500000|\n",
      "|  1002|   Gaven|1991-09-25| Sr Data Engineer|    2019-02-08|    4| 31|        3| 700000|               Y| 840000|\n",
      "|  1016|   Dayna|1999-03-25|Sr Data Scientist|    2021-10-27|    5| 23|        1|1400000|               N|1400000|\n",
      "|  1012| Marques|1986-01-25|Sr Data Scientist|    2017-10-16|    4| 36|        5| 900000|               Y|1080000|\n",
      "|  1009|   Lavar|1994-03-04|    Data Engineer|    2016-02-25|    2| 28|        6| 900000|               Y| 990000|\n",
      "|  1000|    Bunk|1991-05-10|    Data Engineer|    2019-08-10|    1| 31|        3|1500000|               N|1500000|\n",
      "|  1013|Berneice|1988-05-28|    Data Engineer|    2022-10-23|    1| 34|        0|1400000|               N|1400000|\n",
      "|  1007|    Nova|1993-07-05| Sr Data Engineer|    2017-03-21|    5| 29|        5|1000000|               N|1000000|\n",
      "|  1011|    Adah|1994-11-02|Sr Data Scientist|    2017-03-14|    3| 28|        5| 500000|               Y| 575000|\n",
      "|  1005|  Karina|1982-12-11|    Data Engineer|    2013-11-20|    2| 39|        8| 100000|               Y| 110000|\n",
      "|  1001|  Dennis|1993-01-16| Sr Data Engineer|    2020-08-01|    5| 29|        2|1100000|               N|1100000|\n",
      "|  1018|    Kirk|1991-07-23|    Data Engineer|    2014-04-27|    5| 31|        8| 300000|               Y| 375000|\n",
      "|  1020|   Gauge|1992-02-04|    Data Engineer|    2020-01-22|    2| 30|        2|1600000|               N|1600000|\n",
      "|  1015|   Kisha|1989-11-03|   Data Scientist|    2017-03-13|    3| 33|        5| 800000|               Y| 920000|\n",
      "|  1014| Florida|1987-03-27|    Data Engineer|    2013-03-04|    1| 35|        9| 700000|               Y| 735000|\n",
      "|  1019|  Helena|1994-12-19|    Data Engineer|    2020-10-01|    1| 27|        2| 600000|               N| 600000|\n",
      "|  1008|    Nena|1988-12-01|          Manager|    2022-05-16|    4| 33|        0|1900000|               N|1900000|\n",
      "|  1004|    Nola|1993-05-13|          Manager|    2022-09-08|    3| 29|        0|1200000|               N|1200000|\n",
      "|  1006|    Chas|1993-03-21|   Data Scientist|    2022-03-25|    1| 29|        0| 200000|               N| 200000|\n",
      "|  1003|  Aliyah|1993-05-15|Sr Data Scientist|    2016-11-06|    3| 29|        6| 400000|               Y| 460000|\n",
      "+------+--------+----------+-----------------+--------------+-----+---+---------+-------+----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce,isnull,col\n",
    "lvls=df_2.select('level').distinct().rdd.flatMap(lambda x:x).collect()\n",
    "\n",
    "print(lvls)\n",
    "when_expr=[when( ((col('EligibleForBonus')=='Y') & (col('level')==i)),col('salary')+ col('salary')*0.05*i ) for i in lvls]\n",
    "# when_expr \n",
    "df_3=df_2.select('*',coalesce(*when_expr).alias('new_sal'))\\\n",
    "        .withColumn('new_sal',when(isnull(col('new_sal')),col('salary')).otherwise(col('new_sal')).cast('int'))\\\n",
    "\n",
    "df_3.show()\n",
    "df_3.write.csv('output/emp_2022_Q4_dtls',header=True,mode='overwrite')\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26a6e0",
   "metadata": {},
   "source": [
    "**Create a band of age group and categorize salary by age group.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f25acdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---+\n",
      "|age_band|      sal|cnt|\n",
      "+--------+---------+---+\n",
      "|   20-25|165100000|162|\n",
      "|   25-30|296600000|272|\n",
      "|   30-35|275700000|255|\n",
      "|   35-40|281600000|266|\n",
      "|   40-45| 47200000| 45|\n",
      "+--------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3.createOrReplaceTempView('emp_dtls')\n",
    "sql_query=\"\"\"\n",
    "SELECT \n",
    "CASE WHEN age>=20 and age<25 then '20-25'\n",
    "     WHEN age>=25 and age<30 then '25-30'\n",
    "     WHEN age>=30 and age<35 then '30-35'\n",
    "     WHEN age>=35 and age<40 then '35-40'\n",
    "     WHEN age>=40 and age<45 then '40-45'\n",
    "ELSE 'OTHER' END AS age_band\n",
    ",SUM(salary) as sal\n",
    ",COUNT(*) as cnt\n",
    "FROM emp_dtls\n",
    "GROUP BY 1\n",
    "ORDER BY 1,2\n",
    "\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76857cc5",
   "metadata": {},
   "source": [
    "**Avg Salary of the age group**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d4c85e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|age_band|avg_sal|\n",
      "+--------+-------+\n",
      "|   20-25|1019136|\n",
      "|   25-30|1090442|\n",
      "|   30-35|1081177|\n",
      "|   35-40|1058647|\n",
      "|   40-45|1048889|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3.createOrReplaceTempView('emp_dtls')\n",
    "sql_query=\"\"\"\n",
    "SELECT \n",
    "CASE WHEN age>=20 and age<25 then '20-25'\n",
    "     WHEN age>=25 and age<30 then '25-30'\n",
    "     WHEN age>=30 and age<35 then '30-35'\n",
    "     WHEN age>=35 and age<40 then '35-40'\n",
    "     WHEN age>=40 and age<45 then '40-45'\n",
    "ELSE 'OTHER' END AS age_band\n",
    ",CEIL(SUM(salary) /COUNT(*)) as avg_sal\n",
    "FROM emp_dtls\n",
    "GROUP BY 1\n",
    "ORDER BY 1,2\n",
    "\"\"\"\n",
    "spark.sql(sql_query).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b3dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
